#!/usr/bin/env python3
# encoding: utf-8

# Import Python standard libraries
import argparse
import csv
import datetime
from itertools import chain, combinations, islice, tee
from operator import itemgetter
import os.path

# Import external libraries
import networkx as nx
import scipy

# TODO: better/user-defined exponent scalars
# TODO: whether to include suboptimal -- make sure it is working
# TODO: specify how many in the output
# TODO: should the avarage be over everything or just what is reported?


def _pairwise(iterable):
    """
    Internal function for sequential pairwise iteration.

    The function follows the recipe in Python's itertools documentation.

    "s -> (s[0], s[1]), (s[1], s[2]), (s[2], s[3]) ...
    """

    item_a, item_b = tee(iterable)
    next(item_a, None)

    return zip(item_a, item_b)


def read_concepticon(args):
    """
    Read standard Concepticon data.
    """

    with open(os.path.join(args.input, "concepticon.tsv")) as tsvfile:
        reader = csv.DictReader(tsvfile, delimiter="\t")
        data = {
            row['ID'] : row
            for row in reader
        }

    return data

def read_infomap(args):
    """
    Read infomap data as generated by CLICS3.
    """

    with open(os.path.join(args.input, "infomap.tsv")) as tsvfile:
        reader = csv.DictReader(tsvfile, delimiter="\t")
        data = {
            row['concepticon_id'] : row
            for row in reader
        }

    return data

def read_colexifications(args):
    """
    Read colexification data as generated by CLICS3.
    """

    with open(os.path.join(args.input, "colexifications.tsv")) as tsvfile:
        reader = csv.DictReader(tsvfile, delimiter="\t")
        data = [
            {
                'concepticonid_a' : row['concepticonid_a'],
                'concepticonid_b' : row['concepticonid_b'],
                'concepticon_gloss_a' : row['concepticon_gloss_a'],
                'concepticon_gloss_b' : row['concepticon_gloss_b'],
                'families' : int(row['families']),
                'languages' : int(row['languages']),
                'words' : int(row['words']),
            }
            for row in reader]

    return data

def compute_weights(concepticon, infomap, colexifications, args):
    """
    Compute the weights for all colexification pairs.
    """

    # Obtain the maximum number of shared families, languages, and words
    max_families = max([colex['families'] for colex in colexifications])
    max_languages = max([colex['languages'] for colex in colexifications])
    max_words = max([colex['words'] for colex in colexifications])

    # Obtain family, language, and word counts for all concepts
    concepts = set(chain.from_iterable([
        [colex['concepticon_gloss_a'], colex['concepticon_gloss_b']]
        for colex in colexifications
            ]))
    concept_families = {
        concept : max([
            colex['families'] for colex in colexifications if
            concept == colex['concepticon_gloss_a'] or
            concept == colex['concepticon_gloss_b'] ])
        for concept in concepts
    }

    concept_languages = {
        concept : max([
            colex['languages'] for colex in colexifications if
            concept == colex['concepticon_gloss_a'] or
            concept == colex['concepticon_gloss_b'] ])
        for concept in concepts
    }

    concept_words = {
        concept : max([
            colex['words'] for colex in colexifications if
            concept == colex['concepticon_gloss_a'] or
            concept == colex['concepticon_gloss_b'] ])
        for concept in concepts
    }

    # For all colexifications, collect the two concepts involved and
    # compute the weight
    weights = {}
    for colex in colexifications:
        # Cache ids and glosses
        cid_a = colex['concepticonid_a']
        cid_b = colex['concepticonid_b']
        gloss_a = colex['concepticon_gloss_a']
        gloss_b = colex['concepticon_gloss_b']

        # Compute correction ratios
        f_ratio = 1.0 - \
            min([concept_families[gloss_a], concept_families[gloss_b]]) / \
            max([concept_families[gloss_a], concept_families[gloss_b]])
        l_ratio = 1.0 - \
            min([concept_families[gloss_a], concept_families[gloss_b]]) / \
            max([concept_languages[gloss_a], concept_languages[gloss_b]])
        w_ratio = 1.0 - \
            min([concept_words[gloss_a], concept_words[gloss_b]]) / \
            max([concept_words[gloss_a], concept_words[gloss_b]])

        # Compute the weight from family, language, and word counts in
        # relation to the global maximum, also correcting by the user-defined
        # or default exponents
        weight  = ((max_families  - colex['families'])  ** f_ratio) ** \
                (1./args.f_dexp)
        weight += ((max_languages - colex['languages']) ** l_ratio) ** \
                (1./args.l_dexp)
        weight += ((max_words     - colex['words'])     ** w_ratio) ** \
                (1./args.w_dexp)

        # Correct weight if the concepts belong to the same cluster
        if infomap[cid_a]['cluster_name'] == infomap[cid_b]['cluster_name']:
            weight = weight ** args.cluster_exp

        # Store the weight
        key = tuple(sorted([gloss_a, gloss_b]))
        weights[key] = weight

    return weights


# todo: CACHE, exchangin memory for speed (no `graph` query) -- but consider
# birectionality
def comp_weight(path, graph):
    """
    Compute the cumulative weight associated with a path in a graph.
    """

    return sum([
        graph.edges[(edge[0], edge[1])]['weight']
        for edge in _pairwise(path)
        ])


def main(args):
    """
    Main function, reading data and generating output.
    """

    # Read data from (a) Concepticon raw files, (b) informap results,
    # (c) colexification data
    concepticon = read_concepticon(args)
    infomap = read_infomap(args)
    colexifications = read_colexifications(args)

    # Compute the weights for all colexification pairs
    weights = compute_weights(concepticon, infomap, colexifications, args)

    # Build the weighted graph; `networkx` takes care of adding nodes
    graph = nx.Graph()
    for concept_pair, weight in weights.items():
        graph.add_edge(concept_pair[0], concept_pair[1], weight=weight)

    # Output graph
    nx.write_weighted_edgelist(graph, "output/graph.edges", delimiter="\t")
    nx.write_gml(graph, "output/graph.gml")


if __name__ == "__main__":
    # Define the parser for when called from the command-line
    parser = argparse.ArgumentParser(description="Compute semantic shift distances.")
    parser.add_argument(
        "--f_dexp",
        type=float,
        help="Denominator exponent for family count correction (default: 3.0)",
        default=1.0)
    parser.add_argument(
        "--l_dexp",
        type=float,
        help="Denominator exponent for language count correction (default: 2.0)",
        default=1.2)
    parser.add_argument(
        "--w_dexp",
        type=float,
        help="Denominator exponent for word count correction (default: 1.0)",
        default=1.4)
    parser.add_argument(
        "--cluster_exp",
        type=float,
        help="Exponent for same cluster correction (default: 0.9)",
        default=0.9)
    parser.add_argument(
        "--input",
        type=str,
        help="Path to the data directory (default: 'data')",
        default="data")
    parser.add_argument(
        "--output",
        type=str,
        help="Path to the output directory (default: 'data')",
        default="output")
    parser.add_argument(
        "-k",
        type=int,
        help="Maximum number of best paths to collect for each pair (default: 3)",
        default=3)
    parser.add_argument(
        "--search",
        type=int,
        help="Multiplier for the search space of best suboptimal path (default: 5)",
        default=5)
    parser.add_argument(
        '--suboptimal',
        action='store_true',
        help="Whether to search for suboptimal paths (expansive, default: False)")
    ARGS = parser.parse_args()

    main(ARGS)
