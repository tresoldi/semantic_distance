#!/usr/bin/env python3
# encoding: utf-8

# Import Python standard libraries
import argparse
import csv
from itertools import chain, combinations, islice, tee
import os.path

# Import external libraries
import networkx as nx

# TODO: better/user-defined exponent scalars
# TODO: add avarage as reported?
# TODO: look for longer path with intermediate steps in the first path?

def _pairwise(iterable):
    """
    Internal function for sequential pairwise iteration.

    The function follows the recipe in Python's itertools documentation.

    "s -> (s[0], s[1]), (s[1], s[2]), (s[2], s[3]) ...
    """
    
    item_a, item_b = tee(iterable)
    next(item_a, None)

    return zip(item_a, item_b)


def read_concepticon(args):
    """
    Read standard Concepticon data.
    """
    
    with open(os.path.join(args.input, "concepticon.tsv")) as tsvfile:
        reader = csv.DictReader(tsvfile, delimiter="\t")
        data = {
            row['ID'] : row
            for row in reader
        }

    return data

def read_infomap(args):
    """
    Read infomap data as generated by CLICS3.
    """

    with open(os.path.join(args.input, "infomap.tsv")) as tsvfile:
        reader = csv.DictReader(tsvfile, delimiter="\t")
        data = {
            row['concepticon_id'] : row
            for row in reader
        }

    return data

def read_colexifications(args):
    """
    Read colexification data as generated by CLICS3.
    """

    with open(os.path.join(args.input, "colexifications.tsv")) as tsvfile:
        reader = csv.DictReader(tsvfile, delimiter="\t")
        data = [
            {
                'concepticonid_a' : row['concepticonid_a'],
                'concepticonid_b' : row['concepticonid_b'],
                'concepticon_gloss_a' : row['concepticon_gloss_a'],
                'concepticon_gloss_b' : row['concepticon_gloss_b'],
                'families' : int(row['families']),
                'languages' : int(row['languages']),
                'words' : int(row['words']),
            }
            for row in reader]

    return data

def compute_weights(concepticon, infomap, colexifications, args):
    """
    Compute the weights for all colexification pairs.
    """

    # Obtain the maximum number of shared families, languages, and words
    max_families = max([colex['families'] for colex in colexifications])
    max_languages = max([colex['languages'] for colex in colexifications])
    max_words = max([colex['words'] for colex in colexifications])

    # For all colexifications, collect the two concepts involved and
    # compute the weight
    weights = {}
    for colex in colexifications:
        # Cache ids and glosses
        cid_a = colex['concepticonid_a']
        cid_b = colex['concepticonid_b']
        gloss_a = colex['concepticon_gloss_a']
        gloss_b = colex['concepticon_gloss_b']

        # Compute the weight from family, language, and word counts in
        # relation to the global maximum, also correcting by the user-defined
        # or default exponents
        weight = (max_families - colex['families']) ** args.f_exp   + \
                 (max_languages - colex['languages']) ** args.l_exp + \
                 (max_words - colex['words']) ** args.w_exp

        # Correct weight if the concepts belong to the same cluster
        if infomap[cid_a]['cluster_name'] == infomap[cid_b]['cluster_name']:
            weight = weight ** args.cluster_exp

        # Store the weight
        weights[(gloss_a, gloss_b)] = weight

    return weights

def comp_weight(path, graph):
    """
    Compute the cumulative weight associated with a path in a graph.
    """
    
    return sum([
        graph.edges[(edge[0], edge[1])]['weight']
        for edge in _pairwise(path)
        ])


def output_distances(graph, args):
    """
    Output the distances and paths for all potential pairs in the graph.
    """

    # Open output handler and write headers (generated accounting for the
    # requested number of k best paths); we also cache the length of the
    # headers, so we don compute it repeatedly later
    handler = open(os.path.join(args.output, "distances.tsv"), "w")
    headers = ['id', 'concept_a', 'concept_b', 'distance'] + \
        list(chain.from_iterable([
            ["path-%i" % i, "steps-%i" % i, "weight-%i" % i] for i in range(args.k)]
        )) + \
        ["path-so", "steps-so", "weight-so"]
    headers_len = len(headers)
    handler.write("\t".join(headers))
    handler.write("\n")

    # Collect data for all possible combinations, operating on the sorted
    # list of concept glosses; we need to use a counter, instead of the
    # index from the enumeration, as we will skip over combinations with
    # no paths
    row_count = 1
    for comb_idx, comb in enumerate(combinations(sorted(graph.nodes), 2)):
        if comb_idx % 250 == 0:
            print("Processing combination #%i..." % comb_idx)
 
        # Collect args.paths shortest paths for the combination, skipping
        # over if there is no path for the current combination. This will
        # collect a higher number of paths, so we can look for the weight of
        # the best path that does not include the intermediate steps of the
        # single best path
        # TODO: what if the single best is a direct one?
        # TODO: set the multiplier from the command line
        try:
            k_paths = list(islice(
                nx.shortest_simple_paths(graph, comb[0], comb[1], weight='weight'),
                args.k*10))
        except:
            # no path
            continue
            
        # Get the sub-optimal best path without the intermediate steps of
        # the best global path; if no exclude path is found, we will use the
        # score from the worst one we collected
        excludes = k_paths[0][1:-1]
        exclude_paths = [
            path for path in k_paths
            if not any([concept in path for concept in excludes])
        ]
        
        if exclude_paths:
            paths = k_paths[:3] + [exclude_paths[0]]
        else:
            paths = k_paths[:3] + [k_paths[-1]]
    
        # Compute the cumulative weight associated with each path 
        weights = [comp_weight(path, graph) for path in paths]

        # Turn paths and weights into a strings and collect the number of steps
        steps = [str(len(path)-2) for path in paths]
        path_strs = ["/".join(path) for path in paths]
        weights_strs = ["%0.2f" % weight for weight in weights]

        # Build buffer and write
        computed_data = chain.from_iterable(zip(path_strs, steps, weights_strs))
        buf = [
            str(row_count),
            comb[0],
            comb[1],
            "%0.2f" % (sum(weights)/len(weights)), # distance
        ] + list(computed_data)

        # Add empty items to the list if necessary
        buf += [""] * (headers_len - len(buf))

        # Write to handler and update counter
        handler.write("\t".join(buf))
        handler.write("\n")
        row_count += 1

        if comb_idx == 10:
            break

    # Close handler and return
    handler.close()


def main(args):
    """
    Main function, reading data and generating output.
    """

    # Read data from (a) Concepticon raw files, (b) informap results,
    # (c) colexification data
    concepticon = read_concepticon(args)
    infomap = read_infomap(args)
    colexifications = read_colexifications(args)

    # Compute the weights for all colexification pairs
    weights = compute_weights(concepticon, infomap, colexifications, args)

    # Build the weighted graph; `networkx` takes care of adding nodes
    graph = nx.Graph()
    for concept_pair, weight in weights.items():
        graph.add_edge(concept_pair[0], concept_pair[1], weight=weight)

    # Output graph
    nx.write_weighted_edgelist(graph, "output/graph.edges", delimiter="\t")
    nx.write_gml(graph, "output/graph.gml")

    # Output the distance for all possible pairs
    output_distances(graph, args)


if __name__ == "__main__":
    # Define the parser for when called from the command-line
    parser = argparse.ArgumentParser(description="Compute semantic shift distances.")
    parser.add_argument(
        "--f_exp",
        type=float,
        help="Exponent for family count correction (default: 1.0)",
        default=1.0)
    parser.add_argument(
        "--l_exp",
        type=float,
        help="Exponent for language count correction (default: 1.2)",
        default=1.2)
    parser.add_argument(
        "--w_exp",
        type=float,
        help="Exponent for word count correction (default: 1.4)",
        default=1.4)
    parser.add_argument(
        "--cluster_exp",
        type=float,
        help="Exponent for same cluster correction (default: 0.9)",
        default=0.9)
    parser.add_argument(
        "--input",
        type=str,
        help="Path to the data directory (default: 'data')",
        default="data")
    parser.add_argument(
        "--output",
        type=str,
        help="Path to the output directory (default: 'data')",
        default="output")
    parser.add_argument(
        "-k",
        type=int,
        help="Maximum number of best paths to collect for each pair (default: 3)",
        default=3)
    ARGS = parser.parse_args()

    main(ARGS)
